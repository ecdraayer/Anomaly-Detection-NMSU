# -*- coding: utf-8 -*-
"""BOCPD (4).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1evv1fbvqTfmzYS57VSEOQGR2hGzZXJd7
"""

import matplotlib.pyplot as plt
from   matplotlib.colors import LogNorm
import numpy as np
from   scipy.stats import norm

"""============================================================================
Python implementation of Bayesian online changepoint detection for a normal
model with unknown mean parameter. For details, see Adams & MacKay 2007:
    "Bayesian Online Changepoint Detection"
    https://arxiv.org/abs/0710.3742
This code implements the figure in the following blog post:
    http://gregorygundersen.com/blog/2019/08/13/bocd/
Author: Gregory Gundersen
============================================================================"""


#def bocd(data, model, hazard):
    """Return run length posterior using Algorithm 1 in Adams & MacKay 2007.
    """
    # 1. Initialize lower triangular matrix representing the posterior as
    # function of time. Model parameters are initialized in the model class.
    R = np.zeros((T + 1, T + 1))
    R[0, 0] = 1
    message = np.array([1])

    for t in range(1, T + 1):

        # 2. Observe new datum.
        x = data[t - 1]

        # 3. Evaluate predictive probabilities.
        pis = model.pred_prob(t, x)

        # 4. Calculate growth probabilities.
        growth_probs = pis * message * (1 - hazard)

        # 5. Calculate changepoint probabilities.
        cp_prob = sum(pis * message * hazard)

        # 6. Calculate evidence
        new_joint = np.append(cp_prob, growth_probs)

        # 7. Determine run length distribution.
        R[t, :t + 1] = new_joint
        evidence = sum(new_joint)
        R[t, :] /= evidence

        # 8. Update sufficient statistics.
        model.update_statistics(t, x)

        # Setup message passing.
        message = new_joint

    return R

# -----------------------------------------------------------------------------

# Implementation of a Gaussian model with known precision. See Kevin Murphy's
# "Conjugate Bayesian analysis of the Gaussian distribution" for a complete
# derivation of the model:
#
#     https://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf
#
class NormalKnownPrecision:

    def __init__(self, mean0, prec0):
        """Initialize model parameters.
        """
        self.mean0 = mean0
        self.prec0 = prec0
        self.mean_params = np.array([mean0])
        self.prec_params = np.array([prec0])

    def pred_prob(self, t, x):
        """Compute predictive probabilities.
        """
        d = lambda x, mu, tau: norm.pdf(x, mu, 1 / tau + 1)
        return np.array([d(x, self.mean_params[i], self.prec_params[i])
                         for i in range(t)])

    def update_statistics(self, t, x):
        """Update sufficient statistics.
        """
        # `offsets` is just a clever way to +1 all the sufficient statistics.
        offsets = np.arange(1, t + 1)
        new_mean_params = (self.mean_params * offsets + x) / (offsets + 1)
        new_prec_params = self.prec_params + 1
        self.mean_params = np.append([self.mean0], new_mean_params)
        self.prec_params = np.append([self.prec0], new_prec_params)

# -----------------------------------------------------------------------------

def generate_data(mean0, prec0, T, cp_prob):
    """Generate partitioned data of T observations according to constant
    changepoint probability `cp_prob` with hyperpriors `mean0` and `prec0`.
    """
    means = [0]
    data = []
    cpts = []
    for t in range(0, T):
        if np.random.random() < cp_prob:
            mean = np.random.normal(mean0, 1 / prec0)
            means.append(mean)
            cpts.append(t)
        data.append(np.random.normal(means[-1], 1))
    return data, cpts

# -----------------------------------------------------------------------------

def plot_posterior(T, data, R, cpts):
    """Plot data, run length posterior, and groundtruth changepoints.
    """
    fig, axes = plt.subplots(2, 1, figsize=(20, 10))
    ax1, ax2 = axes

    ax1.scatter(range(0, T), data)
    ax1.plot(range(0, T), data)
    ax1.set_xlim([0, T])
    ax1.margins(0)

    norm = LogNorm(vmin=0.0001, vmax=1)
    ax2.imshow(np.rot90(R), aspect='auto', cmap='gray_r', norm=norm)
    ax2.set_xlim([0, T])
    # This just reverses the y-tick marks.
    ticks = list(range(0, T+1, 50))
    ax2.set_yticks(ticks)
    ax2.set_yticklabels(ticks[::-1])
    ax2.margins(0)

    for cpt in cpts:
        ax1.axvline(cpt, c='r', ls='dotted')
        ax2.axvline(cpt, c='r', ls='dotted')

    plt.tight_layout()
    plt.show()

# -----------------------------------------------------------------------------

if __name__ == '__main__':
#    T =         # Number of observations.
    cp_prob = 1/50  # Constant prior on changepoint probability.
    mean0 = 0       # Prior on Gaussian mean.
    prec0 = 0.2     # Prior on Gaussian precision.
    data, cpts = generate_data(mean0, prec0, T, cp_prob)

#     model = NormalKnownPrecision(mean0, prec0)
#     R = bocd(data=data, model=model, hazard=1/50)
#     # The model becomes numerically unstable for large `T` because the mass is
#     # distributed across a support whose size is increasing.
#     for row in R:
#         assert np.isclose(np.sum(row), 1)
#     plot_posterior(T, data, R, cpts)

print(type(data))

data = []
f = open("Synthetic_Sin+anom/varNoise_fixedNumberAnomaliesfixedLength/SinusRW_Length_112000_AnomalyL_200_AnomalyN_60_NoisePerc_5.ts", "r")
data = f.read()
data = (data.rstrip().split('\n')) 
data = [float(i) for i in data]
f.close()
cpts = []
f = open("Synthetic_Sin+anom/varNoise_fixedNumberAnomaliesfixedLength/SinusRW_Length_112000_AnomalyL_200_AnomalyN_60_NoisePerc_5_Annotations.txt", "r")
cpts = f.read()
cpts = (cpts.rstrip().split('\n')) 
cpts = [int(i) for i in cpts]
f.close()

data = data[:1000]

cpts = cpts[:1]

cpts

type(data)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import numpy as np
np.random.seed(555)
import matplotlib.pyplot as plt
import bocd

#f = open("Synthetic_Sin+anom/varNoise_fixedNumberAnomaliesfixedLength/SinusRW_Length_112000_AnomalyL_200_AnomalyN_60_NoisePerc_5.ts", "r")
f = open("RealDatasets/MBA_ECG_801.ts", "r")
test_signal = []
data = f.readlines()
for d in data:
    test_signal.append(float(d))
test_signal = np.asarray(test_signal)
print(test_signal[0])
print(test_signal.shape)
test_signal = test_signal[0:1600]
f.close()

#print(test_signal[:100])
# Generate test data
# test_signal = np.concatenate(
#     [np.random.normal(0.1, 1.0, 300), 
#      np.random.normal(2.0, 0.8, 300),
#      np.random.normal(1.0, 0.5, 300),
#      np.random.normal(3.0, 1.0, 300)])
plt.plot(test_signal)

# Initialize object
bc = BayesianOnlineChangePointDetection(ConstantHazard(200), StudentT(mu=0, kappa=1, alpha=1, beta=1))

# Online estimation and get the maximum likelihood r_t at each time point
rt_mle = np.empty(test_signal.shape)
for i, d in enumerate(test_signal):
    if i >=14:
        #print(i)
        bc.update(test_signal[i-14:i+1])
        rt_mle[i] = bc.rt
        
        #bc.update(test_signal[i])
        #rt_mle[i] = bc.rt
    else:
        bc.update(d)
        rt_mle[i] = 0

alpha = [0.001,1,100]
beta = [0.001,1,100]
kappa = [0.001,1,100]


for a in alpha:
    for b in beta:
        for k in kappa:
            # Initialize object
            bc = BayesianOnlineChangePointDetection(ConstantHazard(200), StudentT(mu=0, kappa=k, alpha=a, beta=b))



            # Online estimation and get the maximum likelihood r_t at each time point
            rt_mle = np.empty(test_signal.shape)
            for i, d in enumerate(test_signal):
                if i >=14:
                    #print(i)
                    bc.update(test_signal[i-14:i+1])
                    rt_mle[i] = bc.rt

                    #bc.update(test_signal[i])
                    #rt_mle[i] = bc.rt
                else:
                    bc.update(d)
                    rt_mle[i] = 0
                    
            print("alpha = ", a, " Beta = ", b, " kappa = ", k)
            plt.plot(rt_mle)
            plt.show()
            # Plot data with estimated change points
            plt.plot(test_signal, alpha=0.5, label="observation")
            index_changes = np.where(np.diff(rt_mle)<-1)[0]
            plt.scatter(index_changes, test_signal[index_changes], c='green', label="change point")
            plt.show()

plt.show()
plt.plot(rt_mle)
plt.show()

# Plot data with estimated change points
plt.plot(test_signal, alpha=0.5, label="observation")
index_changes = np.where(np.diff(rt_mle)<-1)[0]
plt.scatter(index_changes, test_signal[index_changes], c='green', label="change point")

import numpy as np


class BayesianOnlineChangePointDetection:
    def __init__(self, hazard, distribution):
        self.hazard = hazard
        self.distribution = distribution
        self.T = 0
        self.beliefs = np.zeros((1, 2))
        self.beliefs[0, 0] = 1.0

    def reset_params(self):
        self.T = 0
        self.beliefs = np.zeros((1, 2))
        self.beliefs[0, 0] = 1.0

    def _expand_belief_matrix(self):
        rows = np.zeros((1, 2))
        self.beliefs = np.concatenate((self.beliefs, rows), axis=0)

    def _shift_belief_matrix(self):
        self.beliefs[:, 0] = self.beliefs[:, 1]
        self.beliefs[:, 1] = 0.0

    def update(self, x):
        
        
        # Evaluate Predictive Probability (3 in Algortihm 1)
        if type(x) is not np.ndarray:
            self._expand_belief_matrix()

            self.distribution.update_params(x)
            # Update internal state
            #print(len(self.beliefs))

            self.T += 1
        else:
            self._expand_belief_matrix()

            pi_t = self.distribution.pdf(x)
            # Calculate H(r_{t-1})
            h = self.hazard(self.rt)

            # Calculate Growth Probability (4 in Algorithm 1)
            #print(pi_t)
            #print(len(self.beliefs))
            #print(self.beliefs)
            self.beliefs[1 : self.T + 2, 1] = self.beliefs[: self.T + 1, 0] * pi_t * (1 - h)

            # Calculate Changepoint Probabilities (5 in Algorithm 1)
            self.beliefs[0, 1] = (self.beliefs[: self.T + 1, 0] * pi_t * h).sum()

            # Determine Run length Distribution (7 in Algorithm 1)
            self.beliefs[:, 1] = self.beliefs[:, 1] / self.beliefs[:, 1].sum()

            # Update sufficient statistics (8 in Algorithm 8)
            self.distribution.update_params(x[-1])

            # Update internal state
            self._shift_belief_matrix()
            self.T += 1
            
            
        
        
    @property
    def rt(self):
        return np.where(self.beliefs[:, 0] == self.beliefs[:, 0].max())[0]

    @property
    def belief(self):
        return self.beliefs[:, 0]

import numpy as np


class Hazard:
    def __call__(self, *args, **kwargs):
        raise NotImplementedError()


class ConstantHazard(Hazard):
    def __init__(self, _lambda):
        self._lambda = _lambda

    def __call__(self, r):
        """
        Args:
          r: The length of the current run (np.ndarray or scalar)

        Returns:
          p: Changepoint Probabilities(np.ndarray with shape = r.shape)
        """
        if isinstance(r, np.ndarray):
            shape = r.shape
        else:
            shape = 1

        return np.ones(shape) / self._lambda

import numpy as np
from scipy import stats


class Distribution:
    def reset_params(self):
        raise NotImplementedError()

    def pdf(self, x):
        raise NotImplementedError()

    def update_params(self, x):
        raise NotImplementedError()


class StudentT(Distribution):
    """ Generalized Student t distribution 
    https://en.wikipedia.org/wiki/Student%27s_t-distribution#Generalized_Student's_t-distribution

    This setting corresponds to select
      1: Gaussian distribution as a likelihood
      2: normal-Gamma distribution as a prior for Gaussian
    """

    def __init__(self, mu=0, kappa=1, alpha=1, beta=1):
        self.mu0 = np.array([mu])
        self.kappa0 = np.array([kappa])
        self.alpha0 = np.array([alpha])
        self.beta0 = np.array([beta])
        # We need the following lines to prevent "outside defined warning"
        self.muT = self.mu0.copy()
        self.kappaT = self.kappa0.copy()
        self.alphaT = self.alpha0.copy()
        self.betaT = self.beta0.copy()

    def reset_params(self):
        self.muT = self.mu0.copy()
        self.kappaT = self.kappa0.copy()
        self.alphaT = self.alpha0.copy()
        self.betaT = self.beta0.copy()

    def pdf(self, x):
        """ Probability Density Function
        """
        #print(x)
        #print()
        #return stats.t.pdf(x, loc=self.muT, df=2 * self.alphaT, scale=np.sqrt(self.betaT * (self.kappaT + 1) / (self.alphaT * self.kappaT)),)
        probs = []
        for xd in x:
            probs.append(stats.t.pdf(xd, loc=self.muT, df=2 * self.alphaT, scale=np.sqrt(self.betaT * (self.kappaT + 1) / (self.alphaT * self.kappaT)),))
        
        ans = np.zeros(len(probs[0]))
        for pro in probs:
            for i in range(len(pro)):
                ans[i] += pro[i]
        ans /= len(probs[0])
        #print(ans)
        return ans
        #return stats.t.pdf(x, loc=self.muT, df=2 * self.alphaT, scale=np.sqrt(self.betaT * (self.kappaT + 1) / (self.alphaT * self.kappaT)),)

    def update_params(self, x):
        """Update Sufficient Statistcs (Parameters)

        To understand why we use this, see e.g.
        Conjugate Bayesian analysis of the Gaussian distribution, Kevin P. Murphyâˆ—
        https://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf
        3.5 Posterior predictive
        """
        self.betaT = np.concatenate(
            [
                self.beta0,
                (self.kappaT + (self.kappaT * (x - self.muT) ** 2) / (2 * (self.kappaT + 1))),
            ]
        )
        self.muT = np.concatenate([self.mu0, (self.kappaT * self.muT + x) / (self.kappaT + 1)])
        self.kappaT = np.concatenate([self.kappa0, self.kappaT + 1])
        self.alphaT = np.concatenate([self.alpha0, self.alphaT + 0.5])

import sys
print(sys.path)

import pyasn

xx = [0,1,2]
print(xx[1:2])

alpha = np.arange(0, 100, 1)

alpha

